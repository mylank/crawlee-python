---
id: crawlers
title: Crawlers
description: Learn about the different types of crawlers provided by Crawlee and their capabilities.
---

import ApiLink from '@site/src/components/ApiLink';
import CodeBlock from '@theme/CodeBlock';

Crawlers in Crawlee are responsible for managing the lifecycle of web scraping tasks. They handle features such as HTTP requests, parsing, session management, and error handling. Crawlee offers multiple types of crawlers, tailored to different needs and scenarios.

## Overview of Crawlers

Crawlers in Crawlee are designed for flexibility and performance, offering options for:
- HTTP-based crawling for lightweight scraping tasks.
- Browser-based crawling for scenarios requiring JavaScript execution.

### Types of Crawlers

1. **`BasicCrawler`**: A low-level crawler for advanced users to define custom behaviors.
2. **`HttpCrawler`**: Handles HTTP requests and responses without additional parsing.
3. **`BeautifulSoupCrawler`**: Parses HTML/XML content using BeautifulSoup.
4. **`ParselCrawler`**: Uses Parsel for CSS and XPath-based content extraction.
5. **`PlaywrightCrawler`**: Executes JavaScript with real browsers using Playwright.

---

## `BasicCrawler`

<CodeBlock className="language-python">
{`from crawlee.crawlers import BasicCrawler

crawler = BasicCrawler()

@crawler.router.default_handler
async def handle_request(context):
    print(f"Processing: {context.request.url}")
    await context.push_data({"url": context.request.url})

await crawler.run(["https://example.com"])`}
</CodeBlock>

The `BasicCrawler` is the foundation for other crawlers, providing:
- Automatic scaling based on system resources.
- Retries for failed requests.
- Session and proxy rotation.
- Direct storage integration.

It is suitable for advanced use cases requiring a highly customizable crawling process.

---

## `HttpCrawler`

<CodeBlock className="language-python">
{`from crawlee.http_crawler import HttpCrawler

crawler = HttpCrawler()

@crawler.router.default_handler
async def handle_request(context):
    response_body = context.http_response.read()
    await context.push_data({"url": context.request.url, "content": response_body[:100]})

await crawler.run(["https://example.com"])`}
</CodeBlock>

The `HttpCrawler` is optimized for scenarios where you need the raw HTTP response. It uses a dummy parser that directly exposes the response body without additional processing.

---

## `BeautifulSoupCrawler`

<CodeBlock className="language-python">
{`from crawlee.beautifulsoup_crawler import BeautifulSoupCrawler

crawler = BeautifulSoupCrawler()

@crawler.router.default_handler
async def handle_request(context):
    title = context.soup.title.string if context.soup.title else "No title"
    await context.push_data({"url": context.request.url, "title": title})

await crawler.run(["https://example.com"])`}
</CodeBlock>

This crawler integrates with BeautifulSoup to parse HTML and XML content. It is ideal for:
- Extracting data from structured HTML.
- Websites that do not require JavaScript execution.

---

## `ParselCrawler`

<CodeBlock className="language-python">
{`from crawlee.parsel_crawler import ParselCrawler

crawler = ParselCrawler()

@crawler.router.default_handler
async def handle_request(context):
    title = context.selector.css("title::text").get()
    await context.push_data({"url": context.request.url, "title": title})

await crawler.run(["https://example.com"])`}
</CodeBlock>

The `ParselCrawler` uses Parsel for extracting data with CSS selectors or XPath. It is an efficient choice for:
- Web scraping with complex selectors.
- HTML/XML parsing.

---

## `PlaywrightCrawler`

<CodeBlock className="language-python">
{`from crawlee.playwright_crawler import PlaywrightCrawler

crawler = PlaywrightCrawler()

@crawler.router.default_handler
async def handle_request(context):
    title = await context.page.title()
    response_body = (await context.response.text())[:100]
    await context.push_data({"url": context.request.url, "title": title, "snippet": response_body})

await crawler.run(["https://example.com"])`}
</CodeBlock>

The `PlaywrightCrawler` is designed for handling dynamic websites with JavaScript:
- Runs headless browsers using Playwright.
- Captures rendered HTML after JavaScript execution.
- Provides features like infinite scrolling and advanced interactions.

---

Each crawler offers specific advantages and caters to particular web scraping needs. Choose the one that best matches your requirements!
